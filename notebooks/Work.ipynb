{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93bfa560",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import seaborn as sns\n",
    "from scipy.stats import chi2_contingency\n",
    "import matplotlib.colors as colors\n",
    "\n",
    "from sklearn.model_selection import train_test_split  \n",
    "from sklearn.linear_model import LogisticRegression  \n",
    "from sklearn.model_selection import StratifiedKFold, RandomizedSearchCV, cross_val_score, cross_validate\n",
    "\n",
    "from sklearn.metrics import make_scorer, f1_score, roc_auc_score, precision_score, recall_score, silhouette_score\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from scipy.stats import uniform, loguniform, randint\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "\n",
    "pd.set_option(\"display.max_columns\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a8bec3a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Ler do dataframe original\n",
    "df = pd.read_csv(\"../data/dados.csv\", encoding=\"ISO-8859-1\", sep=\";\", low_memory=False)\n",
    "print(f'Linhas, Colunas: {df.shape} \\n')\n",
    "print(f'Informações: \\n')\n",
    "print(f'{df.info()}\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe2b5fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Overview do dataset\n",
    "df.info() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cdb1c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Carrega o dataset de taxas de rendimento\n",
    "df_tx = pd.read_excel(\"../data/tx_rend.xlsx\", skiprows = 8, nrows = 129307)\n",
    "print(f'Linhas, Colunas: {df_tx.shape} \\n')\n",
    "print(f'Informações: \\n')\n",
    "print(f'{df_tx.info()}\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc0e971",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dados os datatypes criados a partir dos dois datasets:\n",
    "df.info()\n",
    "df_tx.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a319946",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar a lista de entidade para cada uma das bases\n",
    "lista_co_entidades_microdados = pd.Series(df['CO_ENTIDADE'].to_list())\n",
    "lista_co_entidades_tx_rend = pd.Series(df_tx['CO_ENTIDADE'].to_list())\n",
    "print(f'O dataset de microdados possui {len(lista_co_entidades_microdados)} códigos de entidade, enquanto o dataset de taxa de rendimentos possui {len(lista_co_entidades_tx_rend)} códigos.\\n')\n",
    "\n",
    "#Obtendo os códigos presentes em um dataset mas ausente em outro\n",
    "codigos_diferentes = [cod for cod in lista_co_entidades_microdados.to_list() if cod not in lista_co_entidades_tx_rend.to_list()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f3dcdbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Total de códigos diferentes\n",
    "len(codigos_diferentes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ca77fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Criando um novo dataframe filtrando os microdados apenas para aqueles que não possuem CO_ENTIDADE que não \n",
    "#tem correspondente na base de taxa de rendimento\n",
    "df_microdados = df.query('CO_ENTIDADE not in @codigos_diferentes').reset_index(drop=True)\n",
    "df_microdados.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f158f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Seleciona as colunas relevantes a partir de seus índices\n",
    "indices_col_tx = [5,45,57]\n",
    "df_tx_filtrado = df_tx.iloc[:,indices_col_tx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed1840d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Renomeia as colunas do Dataframe alvo para ficarem com valores mais claros\n",
    "df_tx_filtrado = df_tx_filtrado.rename(columns={\"3_CAT_FUN\" : \"ABAND_FUND\", \"3_CAT_MED\" : \"ABAND_MED\"})\n",
    "df_tx_filtrado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f76356e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filtra as colunas selecionadas apenas do dataset de microdados:\n",
    "indices_col_microdados = ['CO_ENTIDADE', 'IN_AGUA_POTAVEL', 'IN_ENERGIA_INEXISTENTE', 'IN_ESGOTO_INEXISTENTE', 'IN_BANHEIRO', 'IN_BIBLIOTECA_SALA_LEITURA', 'IN_COZINHA', 'IN_LABORATORIO_CIENCIAS', 'IN_LABORATORIO_INFORMATICA', 'IN_PARQUE_INFANTIL', 'IN_QUADRA_ESPORTES',\n",
    "                         'IN_REFEITORIO', 'IN_LABORATORIO_EDUC_PROF', 'IN_SALA_MULTIUSO', 'IN_SALA_REPOUSO_ALUNO', 'IN_ACESSIBILIDADE_INEXISTENTE', 'QT_SALAS_UTILIZADAS', 'QT_SALAS_UTILIZA_CLIMATIZADAS', 'IN_DESKTOP_ALUNO', 'IN_COMP_PORTATIL_ALUNO', 'IN_TABLET_ALUNO', 'IN_INTERNET', 'IN_INTERNET_APRENDIZAGEM',\n",
    "                         'IN_INTERNET_COMUNIDADE', 'IN_PROF_BIBLIOTECARIO', 'IN_PROF_SAUDE', 'IN_ALIMENTACAO', \n",
    "                          'IN_ESPACO_ATIVIDADE', 'IN_FUND', 'IN_MED']\n",
    "df_microdados_filtrado = df_microdados.loc[:,indices_col_microdados]\n",
    "df_microdados_filtrado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "969b52c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salva os dataframes em arquivos, para um checkpoint\n",
    "df_tx_filtrado.to_pickle(\"../data/tx_aband.pk1\")\n",
    "df_microdados_filtrado.to_feather(\"../data/microdados.feather\")\n",
    "df_tx_filtrado.info()\n",
    "df_microdados_filtrado.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4276531f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Carrega do backup\n",
    "df_tx_use = pd.read_pickle(\"../data/tx_aband.pk1\")\n",
    "df_mc_use = pd.read_feather(\"../data/microdados.feather\")\n",
    "\n",
    "df_microdados = df_mc_use.copy()\n",
    "df_tx = df_tx_use.copy()\n",
    "df_tx.info()\n",
    "df_microdados.info()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b6dea70",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Seleção das colunas binárias\n",
    "columns_dt = ['IN_AGUA_POTAVEL', 'IN_ENERGIA_INEXISTENTE', 'IN_BANHEIRO', 'IN_ACESSIBILIDADE_INEXISTENTE']\n",
    "\n",
    "# Configurar o estilo do seaborn para gráficos mais bonitos\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "# Função para mapear 0 e 1 para 'negativo' e 'positivo'\n",
    "def map_labels(value):\n",
    "    return 'negativo' if value == 0 else 'positivo'\n",
    "\n",
    "# Preparar os dados para o gráfico agrupado\n",
    "df_melted = pd.DataFrame()\n",
    "\n",
    "for col in columns_dt:\n",
    "    temp_df = df_microdados[[col]].copy()\n",
    "    temp_df['Feature'] = col\n",
    "    temp_df['Value'] = temp_df[col].map(map_labels)\n",
    "    df_melted = pd.concat([df_melted, temp_df], axis=0)\n",
    "\n",
    "# Definindo a paleta de cores personalizada\n",
    "palette = {'negativo': 'red', 'positivo': 'green'}\n",
    "\n",
    "# Plotar o gráfico de barras agrupado\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "ax = sns.countplot(data=df_melted, x='Feature', hue='Value', palette=palette, )\n",
    "\n",
    "# Adicionar rótulos de contagem em cima das barras\n",
    "for p in ax.patches:\n",
    "    ax.annotate(f'{p.get_height()}', (p.get_x() + p.get_width() / 2., p.get_height()),\n",
    "                ha='center', va='baseline', fontsize=12, color='black', xytext=(0, 5),\n",
    "                textcoords='offset points')\n",
    "\n",
    "plt.title('Contagem de valores positivos e negativos para cada característica')\n",
    "plt.xlabel('Característica')\n",
    "plt.ylabel('Contagem')\n",
    "plt.legend(title='Valor')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5941d223",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Supondo que df_microdados seja o seu DataFrame original\n",
    "# Seleção das colunas binárias\n",
    "columns_dt = ['IN_LABORATORIO_INFORMATICA', 'IN_DESKTOP_ALUNO',\n",
    "       'IN_COMP_PORTATIL_ALUNO', 'IN_TABLET_ALUNO', 'IN_INTERNET',\n",
    "       'IN_INTERNET_APRENDIZAGEM', 'IN_INTERNET_COMUNIDADE',\n",
    "       ]\n",
    "\n",
    "# Configurar o estilo do seaborn para gráficos mais bonitos\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "# Função para mapear 0 e 1 para 'negativo' e 'positivo'\n",
    "def map_labels(value):\n",
    "    return 'negativo' if value == 0 else 'positivo'\n",
    "\n",
    "# Preparar os dados para o gráfico agrupado\n",
    "df_melted = pd.DataFrame()\n",
    "\n",
    "for col in columns_dt:\n",
    "    temp_df = df_microdados[[col]].copy()\n",
    "    temp_df['Feature'] = col\n",
    "    temp_df['Value'] = temp_df[col].map(map_labels)\n",
    "    df_melted = pd.concat([df_melted, temp_df], axis=0)\n",
    "\n",
    "# Definindo a paleta de cores personalizada\n",
    "palette = {'negativo': 'red', 'positivo': 'green'}\n",
    "\n",
    "# Plotar o gráfico de barras agrupado\n",
    "plt.figure(figsize=(18, 8))\n",
    "\n",
    "ax = sns.countplot(data=df_melted, x='Feature', hue='Value', palette=palette, )\n",
    "\n",
    "# Adicionar rótulos de contagem em cima das barras\n",
    "for p in ax.patches:\n",
    "    ax.annotate(f'{p.get_height()}', (p.get_x() + p.get_width() / 2., p.get_height()),\n",
    "                ha='center', va='baseline', fontsize=12, color='black', xytext=(0, 5),\n",
    "                textcoords='offset points')\n",
    "\n",
    "plt.title('Contagem de valores positivos e negativos para cada característica')\n",
    "plt.xlabel('Característica')\n",
    "plt.ylabel('Contagem')\n",
    "plt.legend(title='Valor')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee24515",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Supondo que df_microdados seja o seu DataFrame original\n",
    "# Seleção das colunas binárias\n",
    "columns_dt = ['IN_BIBLIOTECA_SALA_LEITURA', 'IN_PROF_BIBLIOTECARIO']\n",
    "\n",
    "# Configurar o estilo do seaborn para gráficos mais bonitos\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "# Função para mapear 0 e 1 para 'negativo' e 'positivo'\n",
    "def map_labels(value):\n",
    "    return 'negativo' if value == 0 else 'positivo'\n",
    "\n",
    "# Preparar os dados para o gráfico agrupado\n",
    "df_melted = pd.DataFrame()\n",
    "\n",
    "for col in columns_dt:\n",
    "    temp_df = df_microdados[[col]].copy()\n",
    "    temp_df['Feature'] = col\n",
    "    temp_df['Value'] = temp_df[col].map(map_labels)\n",
    "    df_melted = pd.concat([df_melted, temp_df], axis=0)\n",
    "\n",
    "# Definindo a paleta de cores personalizada\n",
    "palette = {'negativo': 'red', 'positivo': 'green'}\n",
    "\n",
    "# Plotar o gráfico de barras agrupado\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "ax = sns.countplot(data=df_melted, x='Feature', hue='Value', palette=palette, )\n",
    "\n",
    "# Adicionar rótulos de contagem em cima das barras\n",
    "for p in ax.patches:\n",
    "    ax.annotate(f'{p.get_height()}', (p.get_x() + p.get_width() / 2., p.get_height()),\n",
    "                ha='center', va='baseline', fontsize=12, color='black', xytext=(0, 5),\n",
    "                textcoords='offset points')\n",
    "\n",
    "plt.title('Contagem de valores positivos e negativos para cada característica')\n",
    "plt.xlabel('Característica')\n",
    "plt.ylabel('Contagem')\n",
    "plt.legend(title='Valor')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1425ffd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Produz um dataframe unindo a partir da coluna de CO_ENTIDADE\n",
    "df_microdados = pd.merge(df_microdados, df_tx, on=\"CO_ENTIDADE\")\n",
    "df_microdados.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5cf4229",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dividir as bases que tem fund e as que não tem IN_FUND e IN_MED\n",
    "df_microdados_fund = df_microdados.query('IN_FUND == 1').reset_index(drop=True)\n",
    "df_microdados_med = df_microdados.query('IN_MED == 1').reset_index(drop=True)\n",
    "\n",
    "#Remover as colunas de indicador já que a divisão já foi feita\n",
    "df_microdados_fund = df_microdados_fund.drop(['IN_FUND', 'IN_MED', 'ABAND_MED' ], axis =1)\n",
    "df_microdados_med = df_microdados_med.drop(['IN_FUND', 'IN_MED', 'ABAND_FUND' ], axis =1)\n",
    "\n",
    "df_microdados_fund['ABAND_FUND'] = df_microdados_fund['ABAND_FUND'].replace('--', 0)\n",
    "df_microdados_med['ABAND_MED'] = df_microdados_med['ABAND_MED'].replace('--', 0)\n",
    "df_microdados_fund['ABAND_FUND'] = df_microdados_fund['ABAND_FUND'].astype('float')\n",
    "df_microdados_med['ABAND_MED'] = df_microdados_med['ABAND_MED'].astype('float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0947715",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_microdados_fund.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc48208",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_microdados_med.info(max_cols =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "301f57bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Estatísticas básicas das variáveis instituições fundamental\n",
    "df_microdados_fund.describe(include = 'all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b2b628f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Estatísticas básicas das variáveis instituições med\n",
    "df_microdados_med.describe(include = 'all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e41f0c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Analise descritiva para a variável alvo fundamental\n",
    "df_microdados_fund['ABAND_FUND'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9259340d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Analise descritiva para a variável alvo médio\n",
    "df_microdados_med['ABAND_MED'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa69a791",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sns.set_theme()\n",
    "plt.figure(figsize=(24,12))\n",
    "plt.subplot(2,1,1)\n",
    "sns.boxplot(data=df_microdados_fund, x='ABAND_FUND', color='skyblue')\n",
    "plt.title(\"Taxa de abandono fundamental\")\n",
    "plt.xticks(range(0,101,5))\n",
    "\n",
    "plt.subplot(2,1,2)\n",
    "sns.boxplot(data=df_microdados_med, x='ABAND_MED', color='skyblue')\n",
    "plt.title(\"Taxa de abandono médio\")\n",
    "plt.xticks(range(0,101,5))\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22efbcb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lista para armazenar as colunas relevantes\n",
    "relevant_columns_fund = []\n",
    "irelevant_columns_fund = []\n",
    "#Itera sobre as colunas do dataframe para criar uma tabela de contigência entre cada coluna e a variável alvo\n",
    "for column in df_microdados_fund.columns:\n",
    "    contigency_table = pd.crosstab(df_microdados_fund[column],df_microdados_fund['ABAND_FUND'])\n",
    "    _, p, _, _ = chi2_contingency(contigency_table)\n",
    "    \n",
    "    if p < 0.05:\n",
    "        relevant_columns_fund.append(column)\n",
    "    else:\n",
    "        irelevant_columns_fund.append(column)\n",
    "    \n",
    "print(relevant_columns_fund)\n",
    "print(irelevant_columns_fund)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd1a8b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lista para armazenar as colunas relevantes\n",
    "relevant_columns_med = []\n",
    "#Itera sobre as colunas do dataframe para criar uma tabela de contigência entre cada coluna e a variável alvo\n",
    "for column in df_microdados_med.columns:\n",
    "    contigency_table = pd.crosstab(df_microdados_med[column],df_microdados_med['ABAND_MED'])\n",
    "    _, p, _, _ = chi2_contingency(contigency_table)\n",
    "    \n",
    "    if p < 0.05:\n",
    "        relevant_columns_med.append(column)\n",
    "    \n",
    "print(relevant_columns_med)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b29cc37",
   "metadata": {},
   "outputs": [],
   "source": [
    "common_coluns = [i for i in relevant_columns_fund if i in relevant_columns_med]\n",
    "\n",
    "print (f'Colunas em comum: {common_coluns}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d74f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reorganizando os DFs com apenas as colunas relevantes\n",
    "relevant_columns_fund = ['CO_ENTIDADE'] + common_coluns + ['ABAND_FUND']\n",
    "relevant_columns_med = ['CO_ENTIDADE'] + common_coluns + ['ABAND_MED']\n",
    "print(f'Relevant coluns for fund: {relevant_columns_fund} \\n Relevant coluns for med: {relevant_columns_med}')\n",
    "#df_microdados_fund = df_microdados_fund[relevant_columns_fund]\n",
    "#df_microdados_med = df_microdados_med[relevant_columns_med]\n",
    "\n",
    "len(df_microdados_med.columns)\n",
    "#len(df_microdados_fund.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b88a67a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_microdados_fund = df_microdados_fund[relevant_columns_fund]\n",
    "df_microdados_med = df_microdados_med[relevant_columns_med]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8390fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualizando os valores únicos\n",
    "unique_target_fund = sorted(df_microdados_fund['ABAND_FUND'].unique())\n",
    "unique_target_med = sorted(df_microdados_med['ABAND_MED'].unique())\n",
    "\n",
    "print(f'Distinct fund: {unique_target_fund} \\n Distinct med: {unique_target_med}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87fe84f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encontrando os percentis com valores significativos\n",
    "final = {}\n",
    "for i in np.arange(0,1,0.01) :\n",
    "    \n",
    "    if df_microdados_fund['ABAND_FUND'].quantile(i) > 0:\n",
    "        final[i] = df_microdados_fund['ABAND_FUND'].quantile(i)\n",
    "print(final)\n",
    "\n",
    "print(len(final.keys()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b7121e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encontrando os percentis com valores significativos\n",
    "final = {}\n",
    "for i in np.arange(0,1,0.01) :\n",
    "    \n",
    "    if df_microdados_med['ABAND_MED'].quantile(i) > 0:\n",
    "        final[i] = df_microdados_med['ABAND_MED'].quantile(i)\n",
    "print(final)\n",
    "print(len(final.keys()))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0481bf7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Definição dos limites conforme encontrado na análise\n",
    "limites_bins_fund = [-0.1, 0.1, 0.7, 1.8, 4.1, 100]\n",
    "limites_bins_med = [-0.1, 0.2, 2.1, 5.4, 10.6, 100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22dd2661",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Construção dos gráficos para visualizar a distribuição dentro dos dataframes\n",
    "ordinal_classes = ['Muito Baixo', 'Baixo', 'Médio', 'Alto', 'Muito Alto']\n",
    "\n",
    "def gerar_grafico_aband_fund():\n",
    "    \n",
    "    categorias = pd.cut(df_microdados_fund[\"ABAND_FUND\"], limites_bins_fund, labels=ordinal_classes)\n",
    "    cores = ['lightskyblue', 'lightgreen', 'gold','lightcoral', 'darkred']\n",
    "    cmap = colors.LinearSegmentedColormap.from_list(\"\", cores)\n",
    "\n",
    "    df_microdados_fund_agrupado = df_microdados_fund.groupby(categorias).size().reset_index().rename(columns={0: 'CONTAGEM'})\n",
    "    sns.barplot(x='ABAND_FUND', y='CONTAGEM', data=df_microdados_fund_agrupado, hue='ABAND_FUND', palette=cores)\n",
    "\n",
    "    plt.xlabel('Categoria')\n",
    "    plt.ylabel('Número de Entradas')\n",
    "    plt.title('Distribuição por Categoria de ABAND_FUND')\n",
    "\n",
    "def gerar_grafico_aband_med():\n",
    "    \n",
    "    categorias = pd.cut(df_microdados_med[\"ABAND_MED\"], limites_bins_med, labels=ordinal_classes)\n",
    "    cores = ['lightskyblue', 'lightgreen', 'gold','lightcoral', 'darkred']\n",
    "    cmap = colors.LinearSegmentedColormap.from_list(\"\", cores)\n",
    "\n",
    "    df_microdados_fund_agrupado = df_microdados_fund.groupby(categorias).size().reset_index().rename(columns={0: 'CONTAGEM'})\n",
    "    sns.barplot(x='ABAND_MED', y='CONTAGEM', data=df_microdados_fund_agrupado, hue='ABAND_MED', palette=cores)\n",
    "\n",
    "    plt.xlabel('Categoria')\n",
    "    plt.ylabel('Número de Entradas')\n",
    "    plt.title('Distribuição por Categoria de ABAND_MED')\n",
    "\n",
    "\n",
    "# Gerar os gráficos em figuras separadas\n",
    "plt.figure(figsize=(20, 6))  # Definir tamanho da figura (opcional)\n",
    "\n",
    "# Gerar e posicionar o primeiro gráfico\n",
    "subplot1 = plt.subplot(1, 2, 1)  # Criar subplot na posição 1, 2 (linha 1, coluna 1)\n",
    "gerar_grafico_aband_fund()\n",
    "\n",
    "\n",
    "# Gerar e posicionar o segundo gráfico\n",
    "subplot2 = plt.subplot(1, 2, 2)  # Criar subplot na posição 1, 2 (linha 1, coluna 2)\n",
    "gerar_grafico_aband_med()\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Apresentar os gráficos na tela\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e715b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_microdados_fund['CLASSE_ABAND'] = pd.cut(df_microdados_fund['ABAND_FUND'], bins=limites_bins_fund, labels=ordinal_classes)\n",
    "df_microdados_med['CLASSE_ABAND'] = pd.cut(df_microdados_med['ABAND_MED'], bins=limites_bins_med, labels=ordinal_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f911136c",
   "metadata": {},
   "source": [
    "# Regressão Logística\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f408c5ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X = df_microdados_fund.drop(columns=['CO_ENTIDADE', 'ABAND_FUND', 'CLASSE_ABAND'])\n",
    "y = df_microdados_fund['CLASSE_ABAND']\n",
    "\n",
    "# Cria um objeto SMOTE\n",
    "smote = SMOTE()\n",
    "\n",
    "# Gera amostras sintéticas para a classe minoritária\n",
    "X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_resampled = pd.Series(label_encoder.fit_transform(y_resampled))\n",
    "\n",
    "# Parâmetros para o RandomizedSearch\n",
    "param_distributions = {\n",
    "    'logistic__C': loguniform(1e-4, 1e4),\n",
    "    'logistic__penalty': ['l1', 'l2', 'elasticnet', 'none'],\n",
    "    'logistic__solver': ['saga'],  # saga supports l1, l2, and elasticnet\n",
    "    'logistic__l1_ratio': uniform(0, 1)  # Only used if penalty is elasticnet\n",
    "}\n",
    "\n",
    "# Criação do pipeline de pré-processamento e modelo\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('logistic', LogisticRegression(multi_class=\"multinomial\", max_iter=10000, class_weight='balanced'))\n",
    "])\n",
    "\n",
    "print(f'Using pipeline: {pipeline}')\n",
    "# RandomizedSearchCV para LogisticRegression com pipeline\n",
    "random_search = RandomizedSearchCV(pipeline, param_distributions, n_iter=50, cv=5, scoring='f1_weighted')\n",
    "random_search.fit(X_resampled, y_resampled)\n",
    "\n",
    "best_model = random_search.best_estimator_\n",
    "\n",
    "print(f'Melhor modelo encontrado: {best_model}')\n",
    "# Avaliação adicional usando cross_val_score com mais dobras (e.g., 10 dobras)\n",
    "scoring = {\n",
    "    'accuracy': 'accuracy',\n",
    "    'precision_weighted': make_scorer(precision_score, average='weighted'),\n",
    "    'recall_weighted': make_scorer(recall_score, average='weighted'),\n",
    "    'f1_weighted': make_scorer(f1_score, average='weighted'),\n",
    "    'roc_auc_ovr': make_scorer(roc_auc_score, multi_class='ovr', average='weighted', needs_proba=True)\n",
    "}\n",
    "\n",
    "# Stratified K-Fold Cross Validation\n",
    "skfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# Usar cross_val_score para calcular as métricas de desempenho\n",
    "scores = cross_val_score(best_model, X_resampled, y_resampled, cv=skfold, scoring='f1_weighted')\n",
    "\n",
    "# Cálculo das métricas\n",
    "mean_f1 = np.mean(scores)\n",
    "print(\"Média de F1-score:\", mean_f1)\n",
    "\n",
    "# Avaliação de outras métricas\n",
    "results = cross_validate(best_model, X_resampled, y_resampled, cv=skfold, scoring=scoring)\n",
    "\n",
    "mean_accuracy = np.mean(results['test_accuracy'])\n",
    "mean_precision = np.mean(results['test_precision_weighted'])\n",
    "mean_recall = np.mean(results['test_recall_weighted'])\n",
    "mean_auc = np.mean(results['test_roc_auc_ovr'])\n",
    "\n",
    "print(\"Média de Accuracy:\", mean_accuracy)\n",
    "print(\"Média de Precision:\", mean_precision)\n",
    "print(\"Média de Recall:\", mean_recall)\n",
    "print(\"Média de AUC-ROC:\", mean_auc)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab41ea93",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X = df_microdados_fund.drop(columns=['CO_ENTIDADE', 'ABAND_FUND', 'CLASSE_ABAND'])\n",
    "y = df_microdados_fund['CLASSE_ABAND']\n",
    "\n",
    "# Cria um objeto SMOTE\n",
    "smote = SMOTE()\n",
    "\n",
    "# Gera amostras sintéticas para a classe minoritária\n",
    "X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_resampled = pd.Series(label_encoder.fit_transform(y_resampled))\n",
    "\n",
    "# Parâmetros para o RandomizedSearch\n",
    "param_distributions = {\n",
    "    'C': np.logspace(-4, 4, 1000),  # Variação maior para C\n",
    "    'penalty': ['elasticnet'],  # Apenas 'elasticnet' é usado\n",
    "    'solver': ['saga'],  # Apenas 'saga' é usado\n",
    "    'l1_ratio': np.linspace(0, 1, 1000)  # Variação maior para l1_ratio\n",
    "}\n",
    "\n",
    "# RandomizedSearchCV para LogisticRegression\n",
    "random_search = RandomizedSearchCV(LogisticRegression(multi_class=\"multinomial\", max_iter=100000, class_weight='balanced'), param_distributions, n_iter=50, cv=6, scoring='f1_weighted', random_state=42)\n",
    "random_search.fit(X_resampled, y_resampled)\n",
    "\n",
    "best_model = random_search.best_estimator_\n",
    "\n",
    "# Avaliação adicional usando cross_validate com mais dobras (e.g., 10 dobras)\n",
    "scoring = {\n",
    "    'accuracy': 'accuracy',\n",
    "    'precision_weighted': make_scorer(precision_score, average='weighted'),\n",
    "    'recall_weighted': make_scorer(recall_score, average='weighted'),\n",
    "    'f1_weighted': make_scorer(f1_score, average='weighted'),\n",
    "    'roc_auc_ovr': make_scorer(roc_auc_score, multi_class='ovr', average='weighted', needs_proba=True)\n",
    "}\n",
    "\n",
    "# Stratified K-Fold Cross Validation\n",
    "skfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# Usar cross_validate para calcular as métricas de desempenho\n",
    "results = cross_validate(best_model, X_resampled, y_resampled, cv=skfold, scoring=scoring)\n",
    "\n",
    "# Cálculo das médias das métricas\n",
    "mean_accuracy = np.mean(results['test_accuracy'])\n",
    "mean_precision = np.mean(results['test_precision_weighted'])\n",
    "mean_recall = np.mean(results['test_recall_weighted'])\n",
    "mean_f1 = np.mean(results['test_f1_weighted'])\n",
    "mean_auc = np.mean(results['test_roc_auc_ovr'])\n",
    "\n",
    "print(\"Média de Accuracy:\", mean_accuracy)\n",
    "print(\"Média de Precision:\", mean_precision)\n",
    "print(\"Média de Recall:\", mean_recall)\n",
    "print(\"Média de F1-score:\", mean_f1)\n",
    "print(\"Média de AUC-ROC:\", mean_auc)\n",
    "\n",
    "print(best_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f5c46d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X = df_microdados_med.drop(columns=['CO_ENTIDADE', 'ABAND_MED', 'CLASSE_ABAND'])\n",
    "y = df_microdados_med['CLASSE_ABAND']\n",
    "\n",
    "# Cria um objeto SMOTE\n",
    "smote = SMOTE()\n",
    "\n",
    "# Gera amostras sintéticas para a classe minoritária\n",
    "X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_resampled = pd.Series(label_encoder.fit_transform(y_resampled))\n",
    "\n",
    "# Parâmetros para o RandomizedSearch\n",
    "param_distributions = {\n",
    "    'C': np.logspace(-4, 4, 1000),  # Variação maior para C\n",
    "    'penalty': ['elasticnet'],  # Apenas 'elasticnet' é usado\n",
    "    'solver': ['saga'],  # Apenas 'saga' é usado\n",
    "    'l1_ratio': np.linspace(0, 1, 1000)  # Variação maior para l1_ratio\n",
    "}\n",
    "\n",
    "# RandomizedSearchCV para LogisticRegression\n",
    "random_search = RandomizedSearchCV(LogisticRegression(multi_class=\"multinomial\", max_iter=100000, class_weight='balanced'), param_distributions, n_iter=50, cv=6, scoring='f1_weighted', random_state=13)\n",
    "random_search.fit(X_resampled, y_resampled)\n",
    "\n",
    "best_model = random_search.best_estimator_\n",
    "\n",
    "# Avaliação adicional usando cross_validate com mais dobras (e.g., 10 dobras)\n",
    "scoring = {\n",
    "    'accuracy': 'accuracy',\n",
    "    'precision_weighted': make_scorer(precision_score, average='weighted'),\n",
    "    'recall_weighted': make_scorer(recall_score, average='weighted'),\n",
    "    'f1_weighted': make_scorer(f1_score, average='weighted'),\n",
    "    'roc_auc_ovr': make_scorer(roc_auc_score, multi_class='ovr', average='weighted', needs_proba=True)\n",
    "}\n",
    "\n",
    "# Stratified K-Fold Cross Validation\n",
    "skfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=13)\n",
    "\n",
    "# Usar cross_validate para calcular as métricas de desempenho\n",
    "results = cross_validate(best_model, X_resampled, y_resampled, cv=skfold, scoring=scoring)\n",
    "\n",
    "# Cálculo das médias das métricas\n",
    "mean_accuracy = np.mean(results['test_accuracy'])\n",
    "mean_precision = np.mean(results['test_precision_weighted'])\n",
    "mean_recall = np.mean(results['test_recall_weighted'])\n",
    "mean_f1 = np.mean(results['test_f1_weighted'])\n",
    "mean_auc = np.mean(results['test_roc_auc_ovr'])\n",
    "\n",
    "print(\"Média de Accuracy:\", mean_accuracy)\n",
    "print(\"Média de Precision:\", mean_precision)\n",
    "print(\"Média de Recall:\", mean_recall)\n",
    "print(\"Média de F1-score:\", mean_f1)\n",
    "print(\"Média de AUC-ROC:\", mean_auc)\n",
    "\n",
    "print(best_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4727d6d7",
   "metadata": {},
   "source": [
    "# Árvore de Decisão"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f86e1c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_dt = df_microdados_fund.drop(columns=['CO_ENTIDADE', 'ABAND_FUND', 'CLASSE_ABAND'])\n",
    "y_dt = df_microdados_fund['CLASSE_ABAND']\n",
    "\n",
    "# Cria um objeto SMOTE\n",
    "smote_dt = SMOTE()\n",
    "\n",
    "# Gera amostras sintéticas para a classe minoritária\n",
    "X_resampled_dt, y_resampled_dt = smote_dt.fit_resample(X_dt, y_dt)\n",
    "\n",
    "# Encode labels\n",
    "label_encoder_dt = LabelEncoder()\n",
    "y_resampled_dt = pd.Series(label_encoder_dt.fit_transform(y_resampled_dt))\n",
    "\n",
    "# Parâmetros para o RandomizedSearch\n",
    "param_distributions_dt = {\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'max_depth': randint(5, 100),  # Ampliando o intervalo para max_depth\n",
    "    'min_samples_split': randint(2, 100),  # Ampliando o intervalo para min_samples_split\n",
    "    'min_samples_leaf': randint(1, 50),  # Mantendo o intervalo para min_samples_leaf\n",
    "    'max_features': ['auto', 'sqrt', 'log2', None],\n",
    "    'class_weight': ['balanced', None],\n",
    "    'min_impurity_decrease': [0.0, 0.1, 0.2, 0.3],  # Mantendo as opções para min_impurity_decrease\n",
    "    'splitter': ['best', 'random'],  # Mantendo as opções para splitter\n",
    "}\n",
    "\n",
    "# RandomizedSearchCV para DecisionTreeClassifier\n",
    "random_search_dt = RandomizedSearchCV(DecisionTreeClassifier(), param_distributions_dt, n_iter=100, cv=5, scoring='f1_weighted', random_state=13)\n",
    "random_search_dt.fit(X_resampled_dt, y_resampled_dt)\n",
    "\n",
    "best_model_dt = random_search_dt.best_estimator_\n",
    "\n",
    "# Avaliação adicional usando cross_validate com mais dobras (e.g., 10 dobras)\n",
    "scoring_dt = {\n",
    "    'accuracy': 'accuracy',\n",
    "    'precision_weighted': make_scorer(precision_score, average='weighted'),\n",
    "    'recall_weighted': make_scorer(recall_score, average='weighted'),\n",
    "    'f1_weighted': make_scorer(f1_score, average='weighted'),\n",
    "    'roc_auc_ovr': make_scorer(roc_auc_score, multi_class='ovr', average='weighted', needs_proba=True)\n",
    "}\n",
    "\n",
    "# Stratified K-Fold Cross Validation\n",
    "skfold_dt = StratifiedKFold(n_splits=10, shuffle=True, random_state=13)\n",
    "\n",
    "# Usar cross_validate para calcular as métricas de desempenho\n",
    "results_dt = cross_validate(best_model_dt, X_resampled_dt, y_resampled_dt, cv=skfold_dt, scoring=scoring_dt)\n",
    "\n",
    "# Cálculo das médias das métricas\n",
    "mean_accuracy_dt = np.mean(results_dt['test_accuracy'])\n",
    "mean_precision_dt = np.mean(results_dt['test_precision_weighted'])\n",
    "mean_recall_dt = np.mean(results_dt['test_recall_weighted'])\n",
    "mean_f1_dt = np.mean(results_dt['test_f1_weighted'])\n",
    "mean_auc_dt = np.mean(results_dt['test_roc_auc_ovr'])\n",
    "\n",
    "print(\"Média de Accuracy:\", mean_accuracy_dt)\n",
    "print(\"Média de Precision:\", mean_precision_dt)\n",
    "print(\"Média de Recall:\", mean_recall_dt)\n",
    "print(\"Média de F1-score:\", mean_f1_dt)\n",
    "print(\"Média de AUC-ROC:\", mean_auc_dt)\n",
    "\n",
    "print(best_model_dt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c5ec466",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_dt = df_microdados_med.drop(columns=['CO_ENTIDADE', 'ABAND_MED', 'CLASSE_ABAND'])\n",
    "y_dt = df_microdados_med['CLASSE_ABAND']\n",
    "\n",
    "# Cria um objeto SMOTE\n",
    "smote_dt = SMOTE()\n",
    "\n",
    "# Gera amostras sintéticas para a classe minoritária\n",
    "X_resampled_dt, y_resampled_dt = smote_dt.fit_resample(X_dt, y_dt)\n",
    "\n",
    "# Encode labels\n",
    "label_encoder_dt = LabelEncoder()\n",
    "y_resampled_dt = pd.Series(label_encoder_dt.fit_transform(y_resampled_dt))\n",
    "\n",
    "# Parâmetros para o RandomizedSearch\n",
    "param_distributions_dt = {\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'max_depth': randint(5, 100),  # Ampliando o intervalo para max_depth\n",
    "    'min_samples_split': randint(2, 100),  # Ampliando o intervalo para min_samples_split\n",
    "    'min_samples_leaf': randint(1, 50),  # Mantendo o intervalo para min_samples_leaf\n",
    "    'max_features': ['auto', 'sqrt', 'log2', None],\n",
    "    'class_weight': ['balanced', None],\n",
    "    'min_impurity_decrease': [0.0, 0.1, 0.2, 0.3],  # Mantendo as opções para min_impurity_decrease\n",
    "    'splitter': ['best', 'random'],  # Mantendo as opções para splitter\n",
    "}\n",
    "\n",
    "# RandomizedSearchCV para DecisionTreeClassifier\n",
    "random_search_dt = RandomizedSearchCV(DecisionTreeClassifier(), param_distributions_dt, n_iter=100, cv=5, scoring='f1_weighted', random_state=13)\n",
    "random_search_dt.fit(X_resampled_dt, y_resampled_dt)\n",
    "\n",
    "best_model_dt = random_search_dt.best_estimator_\n",
    "\n",
    "# Avaliação adicional usando cross_validate com mais dobras (e.g., 10 dobras)\n",
    "scoring_dt = {\n",
    "    'accuracy': 'accuracy',\n",
    "    'precision_weighted': make_scorer(precision_score, average='weighted'),\n",
    "    'recall_weighted': make_scorer(recall_score, average='weighted'),\n",
    "    'f1_weighted': make_scorer(f1_score, average='weighted'),\n",
    "    'roc_auc_ovr': make_scorer(roc_auc_score, multi_class='ovr', average='weighted', needs_proba=True)\n",
    "}\n",
    "\n",
    "# Stratified K-Fold Cross Validation\n",
    "skfold_dt = StratifiedKFold(n_splits=10, shuffle=True, random_state=13)\n",
    "\n",
    "# Usar cross_validate para calcular as métricas de desempenho\n",
    "results_dt = cross_validate(best_model_dt, X_resampled_dt, y_resampled_dt, cv=skfold_dt, scoring=scoring_dt)\n",
    "\n",
    "# Cálculo das médias das métricas\n",
    "mean_accuracy_dt = np.mean(results_dt['test_accuracy'])\n",
    "mean_precision_dt = np.mean(results_dt['test_precision_weighted'])\n",
    "mean_recall_dt = np.mean(results_dt['test_recall_weighted'])\n",
    "mean_f1_dt = np.mean(results_dt['test_f1_weighted'])\n",
    "mean_auc_dt = np.mean(results_dt['test_roc_auc_ovr'])\n",
    "\n",
    "print(\"Média de Accuracy:\", mean_accuracy_dt)\n",
    "print(\"Média de Precision:\", mean_precision_dt)\n",
    "print(\"Média de Recall:\", mean_recall_dt)\n",
    "print(\"Média de F1-score:\", mean_f1_dt)\n",
    "print(\"Média de AUC-ROC:\", mean_auc_dt)\n",
    "\n",
    "print(best_model_dt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f349f454",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_dt.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b42cbc4b",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be6f8dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Preparação dos dados\n",
    "X_rf = df_microdados_fund.drop(columns=['CO_ENTIDADE', 'ABAND_FUND', 'CLASSE_ABAND'])\n",
    "y_rf = df_microdados_fund['CLASSE_ABAND']\n",
    "\n",
    "# Cria um objeto SMOTE\n",
    "smote_rf = SMOTE()\n",
    "\n",
    "# Gera amostras sintéticas para a classe minoritária\n",
    "X_resampled_rf, y_resampled_rf = smote_rf.fit_resample(X_rf, y_rf)\n",
    "\n",
    "# Encode labels\n",
    "label_encoder_rf = LabelEncoder()\n",
    "y_resampled_rf = pd.Series(label_encoder_rf.fit_transform(y_resampled_rf))\n",
    "\n",
    "# Parâmetros para o RandomizedSearch\n",
    "param_distributions_rf = {\n",
    "    'n_estimators': randint(100, 1000),\n",
    "    'max_depth': randint(10, 50),\n",
    "    'min_samples_split': randint(2, 20),\n",
    "    'min_samples_leaf': randint(1, 20),\n",
    "    'max_features': ['sqrt', 'log2', None],\n",
    "    'bootstrap': [True, False],\n",
    "    'class_weight': ['balanced', 'balanced_subsample', None]\n",
    "}\n",
    "\n",
    "# RandomizedSearchCV para RandomForestClassifier\n",
    "random_search_rf = RandomizedSearchCV(RandomForestClassifier(), param_distributions_rf, n_iter=20, cv=3, scoring='f1_weighted', n_jobs=-1, verbose=3)\n",
    "random_search_rf.fit(X_resampled_rf, y_resampled_rf)\n",
    "\n",
    "best_model_rf = random_search_rf.best_estimator_\n",
    "\n",
    "# Avaliação adicional usando cross_validate com mais dobras (e.g., 10 dobras)\n",
    "scoring_rf = {\n",
    "    'accuracy': 'accuracy',\n",
    "    'precision_weighted': make_scorer(precision_score, average='weighted'),\n",
    "    'recall_weighted': make_scorer(recall_score, average='weighted'),\n",
    "    'f1_weighted': make_scorer(f1_score, average='weighted'),\n",
    "    'roc_auc_ovr': make_scorer(roc_auc_score, multi_class='ovr', average='weighted', needs_proba=True)\n",
    "}\n",
    "\n",
    "# Stratified K-Fold Cross Validation\n",
    "skfold_rf = StratifiedKFold(n_splits=10, shuffle=True)\n",
    "\n",
    "# Usar cross_validate para calcular as métricas de desempenho\n",
    "results_rf = cross_validate(best_model_rf, X_resampled_rf, y_resampled_rf, cv=skfold_rf, scoring=scoring_rf)\n",
    "\n",
    "# Cálculo das médias das métricas\n",
    "mean_accuracy_rf = np.mean(results_rf['test_accuracy'])\n",
    "mean_precision_rf = np.mean(results_rf['test_precision_weighted'])\n",
    "mean_recall_rf = np.mean(results_rf['test_recall_weighted'])\n",
    "mean_f1_rf = np.mean(results_rf['test_f1_weighted'])\n",
    "mean_auc_rf = np.mean(results_rf['test_roc_auc_ovr'])\n",
    "\n",
    "print(\"Média de Accuracy:\", mean_accuracy_rf)\n",
    "print(\"Média de Precision:\", mean_precision_rf)\n",
    "print(\"Média de Recall:\", mean_recall_rf)\n",
    "print(\"Média de F1-score:\", mean_f1_rf)\n",
    "print(\"Média de AUC-ROC:\", mean_auc_rf)\n",
    "\n",
    "print(best_model_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0271f01c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Preparação dos dados\n",
    "X_rf = df_microdados_med.drop(columns=['CO_ENTIDADE', 'ABAND_MED', 'CLASSE_ABAND'])\n",
    "y_rf = df_microdados_med['CLASSE_ABAND']\n",
    "\n",
    "# Cria um objeto SMOTE\n",
    "smote_rf = SMOTE()\n",
    "\n",
    "# Gera amostras sintéticas para a classe minoritária\n",
    "X_resampled_rf, y_resampled_rf = smote_rf.fit_resample(X_rf, y_rf)\n",
    "\n",
    "# Encode labels\n",
    "label_encoder_rf = LabelEncoder()\n",
    "y_resampled_rf = pd.Series(label_encoder_rf.fit_transform(y_resampled_rf))\n",
    "\n",
    "# Parâmetros para o RandomizedSearch\n",
    "param_distributions_rf = {\n",
    "    'n_estimators': randint(100, 1000),\n",
    "    'max_depth': randint(10, 50),\n",
    "    'min_samples_split': randint(2, 20),\n",
    "    'min_samples_leaf': randint(1, 20),\n",
    "    'max_features': ['sqrt', 'log2', None],\n",
    "    'bootstrap': [True, False],\n",
    "    'class_weight': ['balanced', 'balanced_subsample', None]\n",
    "}\n",
    "\n",
    "# RandomizedSearchCV para RandomForestClassifier\n",
    "random_search_rf = RandomizedSearchCV(RandomForestClassifier(), param_distributions_rf, n_iter=20, cv=3, scoring='f1_weighted', n_jobs=-1, verbose=3)\n",
    "random_search_rf.fit(X_resampled_rf, y_resampled_rf)\n",
    "\n",
    "best_model_rf = random_search_rf.best_estimator_\n",
    "\n",
    "# Avaliação adicional usando cross_validate com mais dobras (e.g., 10 dobras)\n",
    "scoring_rf = {\n",
    "    'accuracy': 'accuracy',\n",
    "    'precision_weighted': make_scorer(precision_score, average='weighted'),\n",
    "    'recall_weighted': make_scorer(recall_score, average='weighted'),\n",
    "    'f1_weighted': make_scorer(f1_score, average='weighted'),\n",
    "    'roc_auc_ovr': make_scorer(roc_auc_score, multi_class='ovr', average='weighted', needs_proba=True)\n",
    "}\n",
    "\n",
    "# Stratified K-Fold Cross Validation\n",
    "skfold_rf = StratifiedKFold(n_splits=10, shuffle=True)\n",
    "\n",
    "# Usar cross_validate para calcular as métricas de desempenho\n",
    "results_rf = cross_validate(best_model_rf, X_resampled_rf, y_resampled_rf, cv=skfold_rf, scoring=scoring_rf)\n",
    "\n",
    "# Cálculo das médias das métricas\n",
    "mean_accuracy_rf = np.mean(results_rf['test_accuracy'])\n",
    "mean_precision_rf = np.mean(results_rf['test_precision_weighted'])\n",
    "mean_recall_rf = np.mean(results_rf['test_recall_weighted'])\n",
    "mean_f1_rf = np.mean(results_rf['test_f1_weighted'])\n",
    "mean_auc_rf = np.mean(results_rf['test_roc_auc_ovr'])\n",
    "\n",
    "print(\"Média de Accuracy:\", mean_accuracy_rf)\n",
    "print(\"Média de Precision:\", mean_precision_rf)\n",
    "print(\"Média de Recall:\", mean_recall_rf)\n",
    "print(\"Média de F1-score:\", mean_f1_rf)\n",
    "print(\"Média de AUC-ROC:\", mean_auc_rf)\n",
    "\n",
    "print(best_model_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e4576a7",
   "metadata": {},
   "source": [
    "# K-means"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f935ac",
   "metadata": {},
   "source": [
    "### Método do cotovelo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1eb084c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Preparação dos dados\n",
    "X_kmeans = df_microdados_fund.drop(columns=['CO_ENTIDADE', 'ABAND_FUND', 'CLASSE_ABAND'])\n",
    "\n",
    "# Padronização dos dados\n",
    "scaler = StandardScaler()\n",
    "X_scaled_kmeans = scaler.fit_transform(X_kmeans)\n",
    "\n",
    "# Determinando o número ideal de clusters com o Método do Cotovelo\n",
    "inertia = []\n",
    "K = range(2, 25)\n",
    "for k in K:\n",
    "    kmeans = KMeans(n_clusters=k, verbose=1)\n",
    "    kmeans.fit(X_scaled_kmeans)\n",
    "    inertia.append(kmeans.inertia_)\n",
    "\n",
    "# Plotando o gráfico do Método do Cotovelo\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(K, inertia, 'bx-')\n",
    "plt.xlabel('Número de clusters (k)')\n",
    "plt.ylabel('Inertia')\n",
    "plt.title('Método do Cotovelo para encontrar o número ideal de clusters')\n",
    "plt.show()\n",
    "\n",
    "# Avaliando diferentes números de clusters usando Silhouette Score\n",
    "silhouette_scores = []\n",
    "best_n = None\n",
    "best_score = -2\n",
    "\n",
    "for k in K[1:]:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=13)\n",
    "    kmeans.fit(X_scaled_kmeans)\n",
    "    labels = kmeans.labels_\n",
    "    silhouette_score_value = silhouette_score(X_scaled_kmeans, labels)\n",
    "    silhouette_scores.append(silhouette_score_value)\n",
    "\n",
    "    if silhouette_score_value > best_score:\n",
    "        best_score = silhouette_score_value\n",
    "        best_n = k\n",
    "\n",
    "print(f'Melhor n: {best_n} e melhor score: {best_score}')\n",
    "\n",
    "# Plotando o gráfico do Silhouette Score\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(K[1:], silhouette_scores, 'bx-')\n",
    "plt.xlabel('Número de clusters (k)')\n",
    "plt.ylabel('Silhouette Score')\n",
    "plt.title('Silhouette Score para diferentes números de clusters')\n",
    "plt.show()\n",
    "\n",
    "# Treinando o modelo K-means com o número ideal de clusters\n",
    "num_clusters = best_n  # Substituído pelo número ideal encontrado\n",
    "kmeans = KMeans(n_clusters=num_clusters, random_state=13)\n",
    "kmeans.fit(X_scaled_kmeans)\n",
    "\n",
    "labels_kmeans = kmeans.predict(X_scaled_kmeans)\n",
    "silhouette_avg_kmeans = silhouette_score(X_scaled_kmeans, labels_kmeans)\n",
    "print(\"Média da Pontuação de Silhueta para K-means:\", silhouette_avg_kmeans)\n",
    "\n",
    "# Visualização dos clusters\n",
    "# Utilizando PCA para reduzir a dimensionalidade para 2D\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_scaled_kmeans)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.scatterplot(x=X_pca[:, 0], y=X_pca[:, 1], hue=labels_kmeans, palette='viridis', s=50, alpha=0.6)\n",
    "plt.title('Visualização dos Clusters com K-Means')\n",
    "plt.xlabel('Componente Principal 1')\n",
    "plt.ylabel('Componente Principal 2')\n",
    "plt.legend(title='Clusters')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce4ae11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparação dos dados\n",
    "X_kmeans = df_microdados_med.drop(columns=['CO_ENTIDADE', 'ABAND_MED', 'CLASSE_ABAND'])\n",
    "\n",
    "# Padronização dos dados\n",
    "scaler = StandardScaler()\n",
    "X_scaled_kmeans = scaler.fit_transform(X_kmeans)\n",
    "\n",
    "# Determinando o número ideal de clusters com o Método do Cotovelo\n",
    "inertia = []\n",
    "K = range(2, 25)\n",
    "for k in K:\n",
    "    kmeans = KMeans(n_clusters=k, verbose=1)\n",
    "    kmeans.fit(X_scaled_kmeans)\n",
    "    inertia.append(kmeans.inertia_)\n",
    "\n",
    "# Plotando o gráfico do Método do Cotovelo\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(K, inertia, 'bx-')\n",
    "plt.xlabel('Número de clusters (k)')\n",
    "plt.ylabel('Inertia')\n",
    "plt.title('Método do Cotovelo para encontrar o número ideal de clusters')\n",
    "plt.show()\n",
    "\n",
    "# Avaliando diferentes números de clusters usando Silhouette Score\n",
    "silhouette_scores = []\n",
    "best_n = None\n",
    "best_score = -2\n",
    "\n",
    "for k in K[1:]:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=13)\n",
    "    kmeans.fit(X_scaled_kmeans)\n",
    "    labels = kmeans.labels_\n",
    "    silhouette_score_value = silhouette_score(X_scaled_kmeans, labels)\n",
    "    silhouette_scores.append(silhouette_score_value)\n",
    "\n",
    "    if silhouette_score_value > best_score:\n",
    "        best_score = silhouette_score_value\n",
    "        best_n = k\n",
    "\n",
    "print(f'Melhor n: {best_n} e melhor score: {best_score}')\n",
    "\n",
    "# Plotando o gráfico do Silhouette Score\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(K[1:], silhouette_scores, 'bx-')\n",
    "plt.xlabel('Número de clusters (k)')\n",
    "plt.ylabel('Silhouette Score')\n",
    "plt.title('Silhouette Score para diferentes números de clusters')\n",
    "plt.show()\n",
    "\n",
    "# Treinando o modelo K-means com o número ideal de clusters\n",
    "num_clusters = best_n  # Substituído pelo número ideal encontrado\n",
    "kmeans = KMeans(n_clusters=num_clusters, random_state=13)\n",
    "kmeans.fit(X_scaled_kmeans)\n",
    "\n",
    "labels_kmeans = kmeans.predict(X_scaled_kmeans)\n",
    "silhouette_avg_kmeans = silhouette_score(X_scaled_kmeans, labels_kmeans)\n",
    "print(\"Média da Pontuação de Silhueta para K-means:\", silhouette_avg_kmeans)\n",
    "\n",
    "# Visualização dos clusters\n",
    "# Utilizando PCA para reduzir a dimensionalidade para 2D\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_scaled_kmeans)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.scatterplot(x=X_pca[:, 0], y=X_pca[:, 1], hue=labels_kmeans, palette='viridis', s=50, alpha=0.6)\n",
    "plt.title('Visualização dos Clusters com K-Means')\n",
    "plt.xlabel('Componente Principal 1')\n",
    "plt.ylabel('Componente Principal 2')\n",
    "plt.legend(title='Clusters')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbac32d0",
   "metadata": {},
   "source": [
    "### Random Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d1b282",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparação dos dados\n",
    "X_kmeans = df_microdados_fund.drop(columns=['CO_ENTIDADE', 'ABAND_FUND', 'CLASSE_ABAND'])\n",
    "y_kmeans = df_microdados_fund['CLASSE_ABAND']  # Variável alvo para comparação\n",
    "\n",
    "# Padronização dos dados\n",
    "scaler = StandardScaler()\n",
    "X_scaled_kmeans = scaler.fit_transform(X_kmeans)\n",
    "\n",
    "# Função customizada para calcular Silhouette Score\n",
    "def silhouette_scorer(estimator, X):\n",
    "    labels = estimator.fit_predict(X)\n",
    "    return silhouette_score(X, labels)\n",
    "\n",
    "# Parâmetros para RandomizedSearchCV\n",
    "param_distributions = {'n_clusters': range(3, 20)}\n",
    "\n",
    "# RandomizedSearchCV para KMeans\n",
    "random_search_kmeans = RandomizedSearchCV(\n",
    "    KMeans(n_init=10),\n",
    "    param_distributions,\n",
    "    n_iter=10,\n",
    "    scoring=silhouette_scorer,\n",
    "    verbose=1  # Para acompanhar a execução\n",
    ")\n",
    "random_search_kmeans.fit(X_scaled_kmeans)\n",
    "\n",
    "# Melhor modelo\n",
    "best_kmeans = random_search_kmeans.best_estimator_\n",
    "print(f'Melhor número de clusters: {best_kmeans.n_clusters}')\n",
    "\n",
    "# Score do melhor modelo\n",
    "best_score = random_search_kmeans.best_score_\n",
    "print(f'Silhouette Score do melhor modelo: {best_score}')\n",
    "\n",
    "# Previsão dos clusters\n",
    "labels_kmeans = best_kmeans.predict(X_scaled_kmeans)\n",
    "\n",
    "# Adicionar os labels dos clusters ao DataFrame original\n",
    "df_clusters = df_microdados_fund.copy()\n",
    "df_clusters['Cluster'] = labels_kmeans\n",
    "\n",
    "# Comparar os clusters com a variável alvo\n",
    "cluster_summary = df_clusters.groupby(['Cluster', 'CLASSE_ABAND']).size().unstack(fill_value=0)\n",
    "\n",
    "print(\"Resumo dos clusters comparado com a variável alvo:\")\n",
    "print(cluster_summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667a3c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_search_kmeans.best_score_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 venv",
   "language": "python",
   "name": "venvp3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
